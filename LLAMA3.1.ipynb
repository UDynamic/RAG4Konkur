{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c671321d-230c-443b-8aed-47ed566f1716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  9 02:40:15 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:18:00.0 Off |                   On |\n",
      "| N/A   46C    P0             44W /  400W |   17699MiB /  81920MiB |     N/A      Default |\n",
      "|                                         |                        |              Enabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| MIG devices:                                                                            |\n",
      "+------------------+----------------------------------+-----------+-----------------------+\n",
      "| GPU  GI  CI  MIG |                     Memory-Usage |        Vol|      Shared           |\n",
      "|      ID  ID  Dev |                       BAR1-Usage | SM     Unc| CE ENC DEC OFA JPG    |\n",
      "|                  |                                  |        ECC|                       |\n",
      "|==================+==================================+===========+=======================|\n",
      "|  0    1   0   0  |           17661MiB / 40192MiB    | 56      0 |  4   0    2    0    0 |\n",
      "|                  |                 9MiB / 65535MiB  |           |                       |\n",
      "+------------------+----------------------------------+-----------+-----------------------+\n",
      "|  0    5   0   1  |              25MiB / 19968MiB    | 28      0 |  2   0    1    0    0 |\n",
      "|                  |                 0MiB / 32767MiB  |           |                       |\n",
      "+------------------+----------------------------------+-----------+-----------------------+\n",
      "|  0   13   0   2  |              13MiB /  9728MiB    | 14      0 |  1   0    0    0    0 |\n",
      "|                  |                 0MiB / 16383MiB  |           |                       |\n",
      "+------------------+----------------------------------+-----------+-----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0    1    0    2302468      C   ...ti/.conda/envs/a_vadiati/bin/python      15794MiB |\n",
      "|    0    1    0    2315113      C   .../.conda/envs/adel_kabiri/bin/python        952MiB |\n",
      "|    0    1    0    2361514      C   .../.conda/envs/adel_kabiri/bin/python        610MiB |\n",
      "|    0    1    0    2415116      C   /srv/anaconda3/bin/python                     226MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "151489fc-736c-40d5-8b72-34df86d1d257",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!kill 3032198"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43358971-40b5-42a9-a200-734652dd0ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True\n",
      "Device: NVIDIA A100-SXM4-80GB MIG 1g.10gb\n",
      "Memory allocated: 0.0 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA:\", torch.cuda.is_available())\n",
    "print(\"Device:\", torch.cuda.get_device_name(0))\n",
    "print(\"Memory allocated:\", torch.cuda.memory_allocated()/1024**2, \"MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11ad5ecf-a42d-4b77-9691-12c109076f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/mahriar_gharaghani/.conda/envs/mahriar_gharaghani/lib/python3.12/site-packages (4.57.3)\n",
      "Requirement already satisfied: accelerate in /home/mahriar_gharaghani/.conda/envs/mahriar_gharaghani/lib/python3.12/site-packages (1.12.0)\n",
      "Requirement already satisfied: bitsandbytes in /home/mahriar_gharaghani/.conda/envs/mahriar_gharaghani/lib/python3.12/site-packages (0.48.2)\n",
      "Requirement already satisfied: filelock in /home/mahriar_gharaghani/.conda/envs/mahriar_gharaghani/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/mahriar_gharaghani/.conda/envs/mahriar_gharaghani/lib/python3.12/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/mahriar_gharaghani/.conda/envs/mahriar_gharaghani/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/mahriar_gharaghani/.conda/envs/mahriar_gharaghani/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/mahriar_gharaghani/.conda/envs/mahriar_gharaghani/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/mahriar_gharaghani/.conda/envs/mahriar_gharaghani/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /home/mahriar_gharaghani/.conda/envs/mahriar_gharaghani/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/mahriar_gharaghani/.conda/envs/mahriar_gharaghani/lib/python3.12/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/mahriar_gharaghani/.conda/envs/mahriar_gharaghani/lib/python3.12/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/mahriar_gharaghani/.conda/envs/mahriar_gharaghani/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: psutil in /home/mahriar_gharaghani/.conda/envs/mahriar_gharaghani/lib/python3.12/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/mahriar_gharaghani/.conda/envs/mahriar_gharaghani/lib/python3.12/site-packages (from accelerate) (2.5.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/mahriar_gharaghani/.conda/envs/mahriar_gharaghani/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/mahriar_gharaghani/.conda/envs/mahriar_gharaghani/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/mahriar_gharaghani/.conda/envs/mahriar_gharaghani/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /home/mahriar_gharaghani/.conda/envs/mahriar_gharaghani/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/mahriar_gharaghani/.conda/envs/mahriar_gharaghani/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: networkx in /home/mahriar_gharaghani/.conda/envs/mahriar_gharaghani/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/mahriar_gharaghani/.conda/envs/mahriar_gharaghani/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/mahriar_gharaghani/.conda/envs/mahriar_gharaghani/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/mahriar_gharaghani/.conda/envs/mahriar_gharaghani/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/mahriar_gharaghani/.conda/envs/mahriar_gharaghani/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mahriar_gharaghani/.conda/envs/mahriar_gharaghani/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mahriar_gharaghani/.conda/envs/mahriar_gharaghani/lib/python3.12/site-packages (from requests->transformers) (2025.7.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/mahriar_gharaghani/.conda/envs/mahriar_gharaghani/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "529c2dfe-62cd-477c-9475-6574c813418e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 4-bit quantized model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 02:40:33.513048: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-12-09 02:40:33.536315: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-12-09 02:40:33.542234: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-12-09 02:40:33.558158: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63e5f04df97c41e38dffc6d1e7e0f0d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "load_dotenv(\".env.local\")\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "if HF_TOKEN is None: raise ValueError(\"Missing HF_TOKEN in .env.local\")\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# --- 4-BIT QUANTIZATION CONFIG ---\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "print(\"Loading 4-bit quantized model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    token=HF_TOKEN,\n",
    "    quantization_config=quant_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",   # automatically puts model on your MIG GPU\n",
    ")\n",
    "\n",
    "print(\"Model loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c17554f-7b40-49e2-b43c-5959950c781f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MODEL RESPONSE ===\n",
      "\n",
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a concise helpful assistant.user\n",
      "\n",
      "Explain deep learning in simple words.assistant\n",
      "\n",
      "**What is Deep Learning?**\n",
      "\n",
      "Deep learning is a type of machine learning that uses artificial neural networks with multiple layers to analyze and interpret data. It's like a computer version of the human brain, where each layer processes information and passes it on to the next layer.\n",
      "\n",
      "**Key Components:**\n",
      "\n",
      "1. **Neural Networks:** Inspired by the human brain, these networks consist of interconnected nodes (neurons) that process and transmit information.\n",
      "2. **Layers:** Multiple layers of neurons are stacked on top of each other, allowing the network to learn complex patterns in data.\n",
      "3. **Artificial Intelligence:** Deep learning is a subset of AI that enables computers to learn from data without being explicitly programmed.\n",
      "\n",
      "**How it Works:**\n",
      "\n",
      "1. **Data Collection:** Gather a large dataset (e.g., images, speech, text).\n",
      "2. **Training:** Feed the data into the neural network, which learns to recognize patterns and relationships.\n",
      "3. **Learning:** The network adjusts its connections (weights and biases) to improve its predictions or classifications.\n",
      "4. **Testing:** Evaluate the network's performance on new, unseen data.\n",
      "\n",
      "**Applications:**\n",
      "\n",
      "1. **Image Recognition:** Identify objects, faces, and scenes in images.\n",
      "2. **Speech Recognition:** Transcribe spoken words into text.\n",
      "3. **Natural Language Processing:** Understand and generate human language.\n",
      "4. **Predictive Modeling:** Forecast trends and outcomes in various domains (e.g., stock prices, weather).\n",
      "\n",
      "**Real-World Examples:**\n",
      "\n",
      "1. **Self-Driving Cars:** Use computer vision and deep learning to detect obstacles and navigate roads.\n",
      "2. **Virtual Assistants:** Employ natural language processing to understand voice commands and respond accordingly.\n",
      "3. **Medical Diagnosis:** Analyze medical images and data to detect diseases and predict patient outcomes.\n",
      "\n",
      "In summary, deep learning is a powerful tool that enables computers to learn from data and make decisions, much like the human brain. Its applications are diverse and rapidly expanding, transforming various industries and aspects of our lives.\n"
     ]
    }
   ],
   "source": [
    "# ---- CHAT EXAMPLE ----\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a concise helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain deep learning in simple words.\"}\n",
    "]\n",
    "\n",
    "# Convert messages ‚Üí model prompt\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# ---- GENERATE ----\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=10000,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "response_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"\\n=== MODEL RESPONSE ===\\n\")\n",
    "print(response_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1295f50e-b54d-4036-870d-65c4cd5b4be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chat with Llama-3.1-8B (type 'exit' to quit)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting chat...\n"
     ]
    }
   ],
   "source": [
    "# --- CONVERSATION LOOP USING LLAMA CHAT TEMPLATE ---\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful, concise assistant.\"}\n",
    "]\n",
    "\n",
    "print(\"\\nChat with Llama-3.1-8B (type 'exit' to quit)\\n\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"User: \").strip()\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Exiting chat...\")\n",
    "        break\n",
    "\n",
    "    # Add user's message\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    # Convert conversation to a prompt\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate response\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # The model produces the *full conversation*, so extract only last turn\n",
    "    # (Llama 3 always puts assistant response at the end)\n",
    "    assistant_reply = response.split(\"<|assistant|>\")[-1].strip()\n",
    "\n",
    "    print(f\"\\nAssistant: {assistant_reply}\\n\")\n",
    "\n",
    "    # Add the assistant response to history\n",
    "    messages.append({\"role\": \"assistant\", \"content\": assistant_reply})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3664874c-1ca2-486e-9d37-645cf5d2725c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;214mSystem prompt loaded. Chat with Llama-3.1-8B!\u001b[0m\n",
      "Type 'exit' to quit.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;39mUser1: \u001b[0m congratulate me for finishing this off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m\u001b[38;5;240m------------------------------------------------------------\u001b[0m\n",
      "\u001b[38;5;46mAssistant1:\u001b[0m system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful, concise assistant.user\n",
      "\n",
      "congratulate me for finishing this offassistant\n",
      "\n",
      "Huge congratulations to you for completing whatever challenge or task you've been working on. That's a fantastic achievement. What was it that you've finished?\n",
      "\u001b[1m\u001b[38;5;240m------------------------------------------------------------\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;39mUser2: \u001b[0m use emojies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m\u001b[38;5;240m------------------------------------------------------------\u001b[0m\n",
      "\u001b[38;5;46mAssistant2:\u001b[0m system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful, concise assistant.user\n",
      "\n",
      "congratulate me for finishing this offassistant\n",
      "\n",
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful, concise assistant.user\n",
      "\n",
      "congratulate me for finishing this offassistant\n",
      "\n",
      "Huge congratulations to you for completing whatever challenge or task you've been working on. That's a fantastic achievement. What was it that you've finished?user\n",
      "\n",
      "use emojiesassistant\n",
      "\n",
      "HUGE congratulations to you for completing whatever challenge or task you've been working on üéâüëè. That's a fantastic achievement üéä! You must be feeling proud and relieved üòä. What was it that you've finished?\n",
      "\u001b[1m\u001b[38;5;240m------------------------------------------------------------\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;39mUser3: \u001b[0m exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;214mExiting chat...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "# ANSI Colors\n",
    "RESET   = \"\\033[0m\"\n",
    "BOLD    = \"\\033[1m\"\n",
    "USER    = \"\\033[38;5;39m\"   # Blue\n",
    "ASSIST  = \"\\033[38;5;46m\"   # Green\n",
    "SYSTEM  = \"\\033[38;5;214m\"  # Orange\n",
    "SEPARATOR = f\"{BOLD}\\033[38;5;240m\" + \"-\" * 60 + RESET\n",
    "\n",
    "# Conversation state\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful, concise assistant.\"}\n",
    "]\n",
    "\n",
    "print(f\"{SYSTEM}System prompt loaded. Chat with Llama-3.1-8B!{RESET}\")\n",
    "print(\"Type 'exit' to quit.\\n\")\n",
    "\n",
    "turn = 1\n",
    "\n",
    "while True:\n",
    "    user_input = input(f\"{USER}User{turn}: {RESET}\").strip()\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        print(f\"{SYSTEM}Exiting chat...{RESET}\")\n",
    "        break\n",
    "\n",
    "    # Add user message\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    # Convert to Llama chat prompt\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Model generate\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=2200,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    assistant_reply = decoded.split(\"<|assistant|>\")[-1].strip()\n",
    "\n",
    "    # Print clean formatted response\n",
    "    print(f\"\\n{SEPARATOR}\")\n",
    "    print(f\"{ASSIST}Assistant{turn}:{RESET} {assistant_reply}\")\n",
    "    print(f\"{SEPARATOR}\\n\")\n",
    "\n",
    "    # Add to history\n",
    "    messages.append({\"role\": \"assistant\", \"content\": assistant_reply})\n",
    "\n",
    "    turn += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "106552a3-8e3f-4ac6-8d28-f67e41303987",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Next is the Embedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b627b49-1bd0-44b2-8356-428cb4ada50a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mahriar_gharaghani)",
   "language": "python",
   "name": "mahriar_gharaghani"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
